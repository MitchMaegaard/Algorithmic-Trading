{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Algorithmic Trading.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "uLg3klOcVtHJ",
        "5AWntJ-PMivf",
        "Qo4HQQw5oCov",
        "2VR8bs9pM766",
        "Yw8yEPBfvkQ4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MitchMaegaard/Algorithmic-Trading/blob/master/Algorithmic_Trading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yCeQ2TeTMX8",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "### Stock Market\n",
        "\n",
        "The *stock market* refers to the collection of markets and exchanges where regular activities of buying and selling stocks, which represent ownership claims in a company, take place. The U.S. stock market is comprised of multiple *exchanges*, including the New York Stock Exchange (NYSE), Nasdaq, BATS, and CBOE. Although other financial securities are also traded on these markets, the scope of this project focuses solely on stocks (equities).\n",
        "\n",
        "### Building a Portfolio\n",
        "\n",
        "Briefly, the purpose of trading on the stock market is to make a profit by taking partial ownership in financing one or more companies. This is accomplished by systematically deciding what companies to invest in, and when to \"take your money and run\". A logical investor purchases shares when they think the value will increase (going *long*), and sells shares when they think the value will decrease (*shorting*). Further, an investor can put together a *trading strategy* where they follow a fixed plan of longing and shorting equities. Any given strategy will work to build a *portfolio* consisting of one or more equities, which subsequently can be evaulated in terms both *risk* (probability of success on an investment) and *return* (cumulative gains/losses on investment); the goal in such a system is to maximize returns while minimizing risk, given the understanding that riskier assets have the potential to producing greater profits (and losses).\n",
        "\n",
        "### Algorithmic Trading\n",
        "\n",
        "As a leading stock exchange, the NYSE alone facilitates approximately 1.46 *million* trades per day from about 2,800 different companies, ranging from blue chips to new high-growth companies. Moreover, each company can be represented by a vast array of financial information, including daily *OHLC* market data (open-high-low-close) along with current trading prices, as well as over 900 fundamental indicators, which can give insight to a companies *liquidity* (ability to convert assets to cash), *leverage* (ability to pay long-term debt), and *valuation* (return on investment) properties. Considering all of the available information, an investor has access to over 2.5 million datapoints on the NYSE each day; for an individual looking to invest in the \"right\" company at the \"right\" time, these numbers can be extremely intimidating, and might even turn several away from investment. This is where the power of *algorithmic* trading comes in.  \n",
        "\n",
        "Algorithmic trading is a broad term for a method that has the ability to harness every bit of this information at any given time, and automate the process of buying and selling stocks based on a pre-defined strategy to generate profits at a speed and frequency that is impossible for a human trader. Algorithmic methods can generally be broken down into *momentum investing* and *forecasting*.  \n",
        "\n",
        "#### Momentum Investing\n",
        "\n",
        "Momentum investing is a strategy that capitalizes on the continuance of an existing market trend, where the idea is to buy or hold securities that have shown high returns over a prior time interval, and sell or avoid those with poor returns over the same interval. Momentum investing can further be broken down into three method subcategories inlcuding (i) moving averages, (ii) cross-asset analysis, and (iii) technical indicators.  \n",
        "\n",
        "*Moving averages* employ a simple strategy: select an equity and define short (e.g., 50-day) and long-term (e.g., 200-day) intervals, then compute a *rolling* average for each day, such that the averages will fluctuate slightly with price, though to a lesser extent than they would on a daily basis (because they are averaging for a longer period). Keeping in mind that this is a *momentum* strategy, we are looking for equities that are performing the best at this moment; therefore, it makes sense that a cross of the 50-day average over the 200-day average would signal a buy, and the opposite would signal a sell. Each time period can be modified and validated for improved performance and to better meet an investors' goals for different scenarios.  \n",
        "\n",
        "*Cross-asset analysis* works similarly, in that trades are signaled by the crossing of \"indicator\" lines, but supplemental assets are pulled in from various sources for evaluation. For this method, 2 and 10-year *treasury yield curves* can be used, with longs signaled by the 10-year moving above the 2-year. In a \"normal\" market environment, long-term bonds offer higher rates to compensate for riskiness and inflation that is not included in a short-term bond; concisely, a diminishing gap or cross of the 2-year over 10-year signals a decline in economic growth, and the investor should exit the market.  \n",
        "\n",
        "The third and final category of momentum investing employs *technical indicators*, which are also used to define thresholds at which equities should be traded. Common indicators utilized in trading algorithms include *relative strength index* (RSI), *moving average convergence-divergence* (MACD), *money flow index* (MFI), *stochastic oscillators*, and *Bollinger Bands*. Although the scope of this project does not cover every method, it focuses on a combination of MACD and stochastic osciallator in what is known as the *double cross* strategy. MACD utilizes an *exponential moving average*, which places greater weight on recent data points than a simple moving average. $$EMA_t = (Value_t*\\frac{smoothing}{1+days}) + EMA_{t-1}*(1-\\frac{smoothing}{1+days})$$ By itself, a MACD exceeding the 9-day EMA signals a buy condition. A stochastic oscialltor is extremely useful in a volatile market, and works by comparing current closing price to a range of prices over time, placing more emphasis on tracking the *speed* of change rather than the price. We define prices $C$ (current price), $H_{14}$ (14-day high), and $L_{14}$ (14-day low) to compute $\\%K$ (the current or \"slow\" value of the stochastic oscillator). $$\\%K = 100*(\\frac{C-L_{14}}{H_{14} - L_{14}})$$ The \"fast\" value of the stochastic indicator, $\\%D$, is the 3-period moving average of $\\%K$. Buy signals for this indicator are $\\%K < 20$ or $\\%K > \\%D$ (as long as both are $<80$). However, going long with the double-cross strategy strictly conditions that the stochastic must signal *prior* to the MACD signal, as long as the later indicator occurrs within 2 trading days. On the other hand, the investor should exit the position at the first sign of either indicator turning down.  \n",
        "\n",
        "Although these strategies are not impelmented in this notebook, they were trialed in a separate coding environment called *QuantConnect*, the technicalities of which will be detailed later. It's clear to see that monitoring these strategies on a daily basis for several hundreds of stocks would require an immense amount of time, hence the need for algorithmic trading. On the other hand, specifically in terms of momentum investing, common downfalls include that the exact period of investment, or continued upward momentum, as well as an estimate of how *much* an asset price will increase, if at all, is largely unknown. The strategy has also received backlash from professional investment managers as a reliable method for long-term investing, due to the efficient market hypothesis. Investment duration is of concern to investors because of trading fees charged by brokers; short-term investments (less than 1-year) will accrue more fees, thus decreased returns, due to the frequency of buying and selling transactions, along with captial taxation. To combat these issues, investors turn to mean-reversion and forecasting for long-term strategies, the later of which is covered in the scope of this project.  \n",
        "\n",
        "#### Forecasting\n",
        "\n",
        "Example data points defined by an independent variable, $x_1$, and dependent variable, $Y$, are defined below.  \n",
        "\n",
        "| $x_1$ | $Y$ |\n",
        "|---------|-------------|\n",
        "| 14.2 | 215 |\n",
        "| 16.4 | 325 |\n",
        "| 11.9 | 185 |\n",
        "| 15.2 | 332 |\n",
        "| 18.5 | 406 |\n",
        "\n",
        "\n",
        "We could proceed to plot these points on a graph with both horizontal ($x_1$) and vertical ($Y$) axes. In elementary algebra, a common method for fitting a dependent variable based on some independent variable can be modeled by the regression equation $Y=mx_1+b$, where $m$ is the slope of the regression line and $b$ is some constant that adjusts where the line crosses the y-axis. Ideally, in this type of model we seek to minimize what's known as the adjusted $r^2$, calculated as follows. $$r^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$$ The value of this function takes on a range of values 0 to 1, where values closer to 1 indicate that the independent variable (also known as the predictor variable) accounts for a greater amount of variation in the dependent variable. The benefit in creating a regression line provides us the potential to estimate future data points, either by *interpolation* ($x_1$ values within the current range) or *extrapolation* ($x_1$ values outside of the current range), such that if we are given some $x_1$ value that we have not yet collected before, the model will be able to effectively give an estimated value, $\\hat{Y}$. The underlying idea of this principal extends to *forecasting*, in which a univariate model considers only a single predictor $x_1$ and *future* $x_1$ values become the $Y$ values.  \n",
        "\n",
        "Forecasting relies on extracting historical patterns from *time series* data, and making future projections based on the relative position in history the data coincides with. Briefly, time series data is a sequence of data points collected in successive order. This is directly applicable to the stock market and investment decision making, as we have the ability to track prices and rates at defined intervals for specified durations of time; for example, logging the daily closing price of a security over several days will supply information to the user as to the day of record and the price on that specific day. Sample data points are shown in the table below.  \n",
        "\n",
        "| Day | $x_1$ |\n",
        "|--------|-------------|\n",
        "| 1 | 215 |\n",
        "| 2 | 325 |\n",
        "| 3 | 185 |\n",
        "| 4 | 332 |\n",
        "| 5 | 406 |\n",
        "\n",
        "Although the \"time\" factor used in the example only incrementally counts the number of observations from a starting point, this is commonly a more complex descriptor including year, day, hour, minute, etc. Time series models are significantly more useful when the data is *serially correlated*, or there exists some underlying relationship between an observation and a prior observation (called a *lag(n)* observation, where *n* corresponds to the look-back number of time-intervals). The scope of this project outlines the steps necessary to analyze time series stock market data for these and other types of relationships, extract key insights, and apply decisions to building a high-performing and low-risk porfolio; it does so by creating a comprehensive guide to the steps in time series modeling, the specific steps and details of which can be found below.  \n",
        "\n",
        "### Market Simulation\n",
        "\n",
        "Stock market *simulators* are platforms that allow users to access equity and financial data, ideally in real-time. They are set up such that a programmer can code a trading strategy, then test the strategy performance in a process known as *backtesting*, where starting capital is allocated to place trades on historical data that would have otherwise happened over a pre-defined time interval. This allows users to test various strategies and evaluate them with performance metrics, prior to implementing them in a live-trading scenario where the risk for loss is significantly increased.  \n",
        "\n",
        "#### Backtesting\n",
        "\n",
        "Common metrics utilized to evaluate strategy performance in comparison to benchmark indices include $\\alpha$, $\\beta$, and *sharpe ratio*. In the stock market, *benchmarks* are a predetermined security or set of securities used to compare a given strategy to a long-term buy-and-hold strategy, and are typically taken to be one of the S&P 500, Dow Jones Industrial Average (DJIA), or Nasdaq Composite Index, as these are constructed of a variety of companies on the market and are utilized by professionals to assess the state of the market as a whole. For example, the (inflation-adjusted) historical average annual return on the S&P 500 is around 6%, meaning an investor could expect to make around \\\\$6,000 in the first year on a \\$100,000 initial investment. Therefore, the goal of a successful investment strategy should achieve higher than 6% annual returns. We can use the proposed evaluation metrics defined below to measure the performance of a portfolio built on an investment strategy, while also considering things such as the riskiness of our portfolio.  \n",
        "\n",
        " $$\\alpha = \\frac{Price_{end}+Distribution-Price_{start}}{Price_{start}}$$ $\\alpha$ measures the excess return of an investment or portfolio relative to the return of a benchmark index, where higher values are preferred. An $\\alpha$ value of 1.0 tells us that the portfolio outperformed a benchmark index by 1%, while a value of -1.0 means the portfolio underperformed by 1%.  \n",
        "\n",
        "$$\\beta = \\frac{Cov(R_e, R_m)}{Var(Rm)}$$ $\\beta$ accounts for the volitility of a security or portfolio in comparison to the entire market, where $R_e$ is the return on an individual stock, and $R_m$ is the return on the overall market.  Here, a value of 1.0 indicates that the portfolio has strong correlation with the market, so the investor is taking on no excess risk, but also no excess rewards. A $\\beta$ greater than 1.0 shows increased volatility, and increases both the risk *and* return of a given portfolio.  \n",
        "\n",
        "$$Sharpe = \\frac{R_p-R_f}{\\sigma_p}$$ *Sharpe ratio* measures the volatility in returns (a.k.a. the \"risk-adjusted\" returns), and is typically taken as a leading indicator because of it identifies not only that an investment is achieving return, but also *how* it is achieving return. We define $R_p$ as the return of a portfolio, $R_f$ as the risk-free rate, and $\\sigma_p$ as the standard deviation of a portfolio's excess return.  Overall, a greater sharpe ratio represents a more attractive risk-adjusted return, with values greater than 1.0 being \"acceptable\", and values of 3.0 or higher being \"excellent\".  \n",
        "\n",
        "#### Platforms\n",
        "\n",
        "After exploring various simulators including NinjaTrader, Quantopian and Zipline Live, I ended up using [QuantConnect](https://www.quantconnect.com/) for a variety of reasons. Not only is the account setup free and easy, they also support programming in both C# and Python, as well as backtesting with complete performance metrics. Quantopian is similar in nature, but QuantConnect gains an edge by providing students with accredited email addresses free access to live trading (e.g., the ability to implement a strategy with real money and a brokerage account), with only small fees placed on individual trades.  \n",
        "\n",
        "Getting started in QuantConnect is relatively straightforward. They have fantastic community support including access to [tutorials](https://www.quantconnect.com/tutorials/home/home), a live discussion forum, several example algorithms, and complete [documentation](https://www.quantconnect.com/docs/home/home). They also support both *research* and *laboratory* environments, where users can perform exploratory data analysis in a Jupyter notebook prior to migrating their strategy to a python script that connects with the backtesting platform. Additonally, *QuantQuote* provides tick data, down to minute resolution, for 29,000 symbols since January 1998, and *Morning Star* provides fundamentals and over 9,000 indicators for 8,000 symbols.  \n",
        "\n",
        "### Why Google Colab?\n",
        "\n",
        "I migrated coding and workflow to Google Colab, as QuantConnect only supported packages for basic data analysis and feature engineering, and made it difficult to import external data or perform web scraping. This wasn't an issue when implementing momentum investing strategies, but the platform struggled to support any types of forecasting packages. Additionally, basic algorithms would often take several minutes to initialize and backtest, as multiple customer scripts through the site were running simultaneously on a single server. Although this would not be an issue for deploying a final algorithm that would only be run once per month, it is not ideal for testing various implementations over a short period of time.  \n",
        "\n",
        "Colaboratory is a Google project aimed at providing a collaborative programming environment, specifically for data science/machine learning education and research. It is a Jupyter notebook environment that runs in the cloud, and each session is equipped with a virtual machine running 13 GB of RAM with the option of a CPU, GPU (NVIDIA Tesla K80), or TPU processor. This makes it easier for users to utilize packages and dependencies, as they do not have to be installed locally. Addionally, work can be easily shared with other users; not only can the file be pushed directly to GitHub, but the notebook itself can actually be shared with other developers who can see output and modify cells in real-time.  \n",
        "\n",
        "To toggle processing units, click on \"Change runtime type\" under the Runtime tab at the top of the notebook, and make a selection under \"Hardware accelerator\". By default the session runs on CPU (\"None\"), but alternative choices can be made accordingly.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLg3klOcVtHJ",
        "colab_type": "text"
      },
      "source": [
        "## Setup Research Environment\n",
        "\n",
        "We need to bring in all of the \"tools\" necessary for data processing, data visualization, model building, and evaluation metrics. Google Colab runs virtually, meaning that when a session is set up, all users have access to the same exact environment. Some packages have additional dependencies on newer versions of other packages; therefore, to ensure consistency and replicability in future use and across platforms, we must make any necessary installations (to the virtual machine) prior to implementing them (to the notebook itself)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4UR3KOFVzxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install numpy --upgrade # needed for new pandas\n",
        "# need 24. > for pmdarima\n",
        "!pip3 install pandas --upgrade\n",
        "!pip3 install statsmodels --upgrade\n",
        "!pip install pmdarima # auto arima"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBakVWc3Vvz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "\n",
        "# data processing\n",
        "import pandas as pd\n",
        "import pandas_datareader as pdr\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "from datetime import datetime, timedelta\n",
        "from random import random\n",
        "\n",
        "# model building and evaluation\n",
        "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "from statsmodels.tsa.arima_model import ARIMA, ARIMAResults\n",
        "from scipy.optimize import brute\n",
        "from pmdarima.arima import auto_arima\n",
        "\n",
        "from sklearn import linear_model as lm\n",
        "\n",
        "from fbprophet import Prophet\n",
        "from fbprophet.diagnostics import cross_validation, performance_metrics\n",
        "\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from pandas.plotting import lag_plot\n",
        "from fbprophet.plot import plot_cross_validation_metric\n",
        "\n",
        "# web scraping\n",
        "import bs4 as bs\n",
        "import pickle\n",
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AWntJ-PMivf",
        "colab_type": "text"
      },
      "source": [
        "## Data and Error Definitions\n",
        "\n",
        "Because the research envrionment has moved away from QuantConnect where security data was provided free of charge or effort, we must pull in historical data for securities manually. Although there are several sources available for users to subscribe and pay for API's to extract more current second-level market data, the easiest method I found suitable for data exploration and modeling was via Yahoo finance, which is incorporated into pandas `DataReader`. By specifying the symbol of a security we're interested, in addition to the desired start and end dates, we can harness daily OHLC (open, high, low, close) prices for the security, as well as trading volume and adjusted closing price (closing price that takes into account dividends, stock splits, etc.).  \n",
        "\n",
        "Additionally, market capitalization can be calculated by multiplying the share price and the number of shares outstanding. This feature is added to create filters that can break stocks down into small (\\\\$300M to \\$2B), mid (\\$2B to \\$10B), large (\\$10B to \\\\$200B), and mega-cap (\\$200B+) bins. For reference, smaller-cap stocks are typically more volatile and therefore riskier, but also present greater growth potential, than do larger-cap stocks. The ability to filter companies by this information is incredibly beneficial for individualizing investors' goals, and is utilized in limiting the total number of companies our forecasting methods will need to search through, thus minimizing overall computation time.  \n",
        "\n",
        "All baseline analyses are performed on the S&P 500 index, which is constructed on the market capitalizations of 500 large American companies. Similar to the strategy implemented in momentum investing, this index is utilized not only for it's relative consistency in comparison to individual stocks because of the diversity, but also because it is often utilized to represent the performance of the market as a whole. The end goal will be to trial various forecating methods on the S&P, then extend the best performing model to the rest of the market.  \n",
        "\n",
        "To ensure consistency and that no additional bias is introduced in modeling, pre-determined start and end dates are chosen as Oct. 31, 2015 and Sep. 1, 2018, respectively. Though somewhat arbitrary, the end date is set prior to downturn of the market in the final quarter of 2018 that carried into the beginning of 2019. Although the final model will need to consider these types of events, the reduced volatility is easier to build predictions on. Each date is also set such that an entire month is accounted for, in the event of correlation between stock prices on corresponding days of each month. Finally, the data collection period runs approximately two years, which should be sufficient for implementing cross-validated forecasting procedures.  \n",
        "\n",
        "In momentum investing, algorithms are evaluated through backtesting where high values of *Sharpe ratio* are preferred. Because we are concerned about the *accuracy* of future predictions over the relative performance of the algorithm in comparison to some baseline, the approach for optimizing models and final model selection in forecasting is achieved somewhat differently. Instead, we primarily use *root mean square error* (RMSE) to compare our predictions ($\\hat{Y}$) with actual future values ($Y$). $$RMSE = \\sqrt{MSE(\\hat{Y})} = \\sqrt{E((\\hat{Y}-Y)^2)} = \\sqrt{\\frac{\\sum_{i=1}^{n}(\\hat{Y}_i-Y_i)^2}{n}}$$ By calculation, RMSE monitors model performance over *n* future timesteps, and places a heavier weight on large errors to ensure that models are performing within a desired boundary of accuracy. Because the metric considers the difference between estimated and actual values, we interpret RMSE of 0 as having no error, and no upper-bound limit. For evaluation, we set $n=63$ future values to correspond with one quarter (approximately three months) of actual trading days (Monday through Friday). Models are build on historcial data known as the *training set*, and the prediction period is artifically simulated by creating a hold-out set, known as the *test set*. This is an important step such that we are not immediately throwing a model into production and having to evaluate performance in real-time.  \n",
        "\n",
        "To ensure consistency, additional metrics to evaluate overall portfolio performance over the defined forecasting period are utilized. These consist of an *annualized Sharpe ratio*, computed by multiplying the Sharpe ratio by $\\sqrt{252}$ (total trading days in one year) and considering a daily risk-free rate of 0, as well as *mean absolute percentage error* (MAPE) calculated as follows. $$MAPE = \\frac{100\\%}{n}\\sum_{i=1}^n\\mid\\frac{Y_i-\\hat{Y}_i}{Y_i}\\mid$$ MAPE is a measure of prediction *accuracy* rather than error, making it easier to present and interpret to a broader audience. Intuitively, a value of 0 means that our model achieves 100\\% accuracy in predicting prices for each of the 63 future trading days."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jqWrwKtMzYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_stock(ticker, start_date, end_date):\n",
        "  # read in data\n",
        "  data = pdr.data.DataReader(ticker, start=start_date, end=end_date, data_source='yahoo')\n",
        "  data['mktcap'] = data['Volume'] * data['Close']\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWVIj-zLn4Jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_sp500_tickers(site='http://en.wikipedia.org/wiki/List_of_S%26P_500_companies'):\n",
        "    resp = requests.get(site)\n",
        "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
        "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
        "    tickers = []\n",
        "    for row in table.findAll('tr')[1:]:\n",
        "        ticker = row.findAll('td')[1].text\n",
        "        tickers.append(ticker)\n",
        "        \n",
        "    with open(\"sp500tickers.pickle\",\"wb\") as f:\n",
        "        pickle.dump(tickers,f)\n",
        "        \n",
        "    return tickers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whvzpjKAM_Jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make data definitions\n",
        "ticker, start, end = 'SPY', '2015-10-31', '2018-9-1'\n",
        "month, quarter, year = 21, 63, 252\n",
        "stock_data = load_stock(ticker, start, end)['Close']\n",
        "stock_data.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfbJC7VavsO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile all performance metrics\n",
        "def percent_change(data):\n",
        "  current, future = data[0], data[len(data)-1]\n",
        "  return 100*(future-current)/current\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "  y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "  return np.mean(np.abs((y_true - y_pred) / y_true))*100\n",
        "\n",
        "def sharpe_ratio(value):\n",
        "  # greater values reflect a more attractive risk-adjusted return\n",
        "  # (expected reuturns - risk free returns) / standard deviation -- assume \"risk-free\" rate to be 0 on a daily basis\n",
        "  returns = np.diff(value) # don't worry about filling NAs\n",
        "  sharpe = returns.mean() / returns.std()\n",
        "  return sharpe * np.sqrt(year) # annualized ratio\n",
        "\n",
        "def performance_stats(true, pred):\n",
        "  return({\"Actual Returns\":percent_change(true), \"Estimated Returns\":percent_change(pred),\n",
        "          \"Projected Sharpe\":sharpe_ratio(pred),\n",
        "          \"RMSE\":np.sqrt(mean_squared_error(true, pred)), \"MAPE\":mean_absolute_percentage_error(true,pred)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91mdCgO7vi-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ts_split(data, pred_period=quarter, print_sets=False):\n",
        "  train_size = len(data) - pred_period\n",
        "  train, test = data[:train_size], data[train_size:]\n",
        "  \n",
        "  if print_sets:\n",
        "    print(\"Total daily observations: {}\".format(len(data)))\n",
        "    print(\"Train set: {num} ({first} to {last})\".format(num=len(train), first=train.index[1].date(), last=train.index[-1].date()))\n",
        "    print(\"Test set: {num} ({first} to {last})\".format(num=len(test), first=test.index[1].date(), last=test.index[-1].date()))\n",
        "  \n",
        "  return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo4HQQw5oCov",
        "colab_type": "text"
      },
      "source": [
        "## Visualize time series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q464fy6anwiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "stock_data[:-quarter].plot(label=\"Train Data\")\n",
        "stock_data[-quarter:].plot(label=\"Test Data\", color='forestgreen', alpha=0.8)\n",
        "plt.axvline(x=stock_data.index[-quarter], linestyle=':', color='forestgreen')\n",
        "plt.title(\"Historical Closing Prices for S&P 500 Index (SPY)\", fontsize=14)\n",
        "plt.ylabel(\"Closing Price ($)\")\n",
        "plt.legend(fontsize='large')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VR8bs9pM766",
        "colab_type": "text"
      },
      "source": [
        "## Make Stationary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eag6ZBY0rW_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stationary_test(data, window=7):\n",
        "  # quick function where we can pass in data and get both a visual & stats test result on stationarity\n",
        "  \n",
        "  roll_avg = data.rolling(window=window, center=False).mean()\n",
        "  roll_std = data.rolling(window=window, center=False).std()\n",
        "  \n",
        "  # plot data\n",
        "  plt.figure(figsize=(20,10))\n",
        "  orig = plt.plot(data, color='blue', label='Original', alpha=0.5)\n",
        "  r_avg = plt.plot(roll_avg, color='red', label='Mean')\n",
        "  r_std = plt.plot(roll_std, color='green', label='Std')\n",
        "  plt.legend(loc='best', fontsize='large')\n",
        "  plt.title('{}-day Rolling Mean & Standard Deviation'.format(window), fontsize=15)\n",
        "  plt.show(block=False)\n",
        "  \n",
        "  # Dickey-Fuller test\n",
        "  df_test = adfuller(data)\n",
        "  df_out = pd.Series(df_test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        "  for key, value in df_test[4].items():\n",
        "    df_out['Critical Value ({k})'.format(k=key)] = value\n",
        "  \n",
        "  print(\"Results of Dickey-Fuller Test:\\n{res}\".format(res=df_out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TXM831yrd17",
        "colab_type": "text"
      },
      "source": [
        "#### Original Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waZcwcH1rwOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stationary_test(stock_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk9kuEOtr0WO",
        "colab_type": "text"
      },
      "source": [
        "#### Log-Transformed Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM3hVSipr8d2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stabilize variance\n",
        "log_data = np.log(stock_data)\n",
        "stationary_test(log_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuyopXQgr-fB",
        "colab_type": "text"
      },
      "source": [
        "#### Differenced Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9vNeSnRsHQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stabilize mean\n",
        "log_diff = np.log(stock_data).diff()\n",
        "log_diff.dropna(inplace=True)\n",
        "stationary_test(log_diff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oymZEDMvsbns",
        "colab_type": "text"
      },
      "source": [
        "#### Reforming Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9c6KXWKsjFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reform_data(data, transf):\n",
        "  # transforms np.log(data).diff() back to it's original form\n",
        "  first_val = [np.log(data[0])]\n",
        "  restructured = np.concatenate((first_val, transf))\n",
        "  return np.exp(restructured.cumsum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPAOZSN8smt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#o, l, d = stock_data[-quarter:], log_data[-quarter:], log_diff[-quarter:]\n",
        "\n",
        "plt.figure(figsize=(20,15))\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(stock_data[-quarter:].values)\n",
        "plt.title(\"Figure 1. Closing Prices\")\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(log_data[-quarter:].values)\n",
        "plt.title(\"Figure 2. log(Close)\")\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "plt.plot(log_diff[-quarter:].values)\n",
        "plt.title(\"Figure 3. diff(log(Close))\")\n",
        "\n",
        "#r = reform_data(stock_data, log_diff)[-quarter:]\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "plt.plot(reform_data(stock_data, log_diff)[-quarter:])\n",
        "plt.title(\"Figure 4. Reformed Closing Prices\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw8yEPBfvkQ4",
        "colab_type": "text"
      },
      "source": [
        "## Autocorrelation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVmUHTSUWukZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(2,2,1) # (numrows, numcols, fignum)\n",
        "lag_plot(log_data, lag=1, c='steelblue', alpha=0.5)\n",
        "plt.title(\"Autocorrelation of log(close)\")\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "lag_plot(log_diff, lag=1, c='steelblue', alpha=0.5)\n",
        "plt.title(\"Autocorrelation of first difference of log(close)\")\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "log_data.plot(kind='kde')\n",
        "plt.title(\"Distribution of log(close)\")\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "log_diff.plot(kind='kde')\n",
        "plt.title(\"Distribution of first difference of log(close)\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd-wBdJuW2wT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_ap(data, num_lags=10):\n",
        "  plt.figure(figsize=(20,10))\n",
        "  plt.subplot(221)\n",
        "  plot_acf(data, ax=plt.gca(), lags=num_lags)\n",
        "  plt.subplot(222)\n",
        "  plot_pacf(data, ax=plt.gca(), lags=num_lags)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWBbMDz2XBSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_lags(log_diff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAt48bs4XZbW",
        "colab_type": "text"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjRfAjnsXbo0",
        "colab_type": "text"
      },
      "source": [
        "### ARIMA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcENfQJqoVO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_arima(data, p_vals, d_val, q_vals):\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "  low_bic, opt_order = np.inf, None\n",
        "  for p in p_vals:\n",
        "    for q in q_vals:\n",
        "      order = (p, d_val, q)\n",
        "      try:\n",
        "        model = ARIMA(data, order).fit()\n",
        "        bic = model.bic\n",
        "        if bic < low_bic:\n",
        "          low_bic, opt_order = bic, order\n",
        "        # for logging: print(\"ARIMA{order}: {bic}\".format(order=order, bic=bic))\n",
        "      except: continue\n",
        "  #print(\"Best ARIMA{order}: {bic}\".format(order=opt_order, bic=round(low_bic, 4)))\n",
        "  return opt_order"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5bIdj7IoVyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def arima_model(data, p, d, q, pred_period=91, persistence=True):\n",
        "  train, test = ts_split(data, pred_period)\n",
        "  \n",
        "  order = optimize_arima(data, p, d, q)\n",
        "  \n",
        "  if order == (0,1,0):\n",
        "    # random walk model\n",
        "    hist, preds = train[-1], list()\n",
        "    for i in range(len(test)):\n",
        "      if persistence: preds.append(hist)\n",
        "      # generate a small amount of random noise and add it to yesterday's closing value\n",
        "      else: preds.append(hist + (-0.001 if random() < 0.5 else 0.001))\n",
        "      hist = test[i]\n",
        "  else:\n",
        "    hist, preds = [x for x in train], list()\n",
        "    for i in range(len(test)):\n",
        "      mod = ARIMA(hist, order).fit(disp=0) # build model\n",
        "      preds.append(mod.forecast()[0]) # predict 1 point into future\n",
        "      hist.append(test[i]) # add values we look at to the train set\n",
        "  \n",
        "  true, pred = np.exp(test.values), np.exp(preds)\n",
        "  stats = performance_stats(true, pred)\n",
        "  \n",
        "  plt.figure(figsize=(22,8))\n",
        "  plt.plot(true, label=\"Acutal\")\n",
        "  plt.plot(pred, color=\"red\", alpha=0.7, label=\"Predicted\")\n",
        "  plt.title(\"ARIMA{} Closing Prices\".format(order), fontsize=15)\n",
        "  plt.ylabel(\"Closing price ($)\", fontsize=12)\n",
        "  plt.xlabel(\"Days after last historical price\", fontsize=12)\n",
        "  plt.legend(fontsize='large')\n",
        "  plt.show()\n",
        "  \n",
        "  for key, value in stats.items():\n",
        "    print(\"{}: {:.4f}\".format(key, value))\n",
        "    \n",
        "  return stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxSaEpk2Xq1f",
        "colab_type": "text"
      },
      "source": [
        "#### Persistence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIK8Gv-boaeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "persistence = arima_model(log_data, range(0,3), 1, range(0,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlCi5dkrXuwX",
        "colab_type": "text"
      },
      "source": [
        "#### Random Walk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3XBHeUFobX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_walk = arima_model(log_data, range(0,3), 1, range(0,3), persistence=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7kjUau-X8Zj",
        "colab_type": "text"
      },
      "source": [
        "#### Auto-ARIMA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXnGcpwqow3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auto = auto_arima(train, start_p=1, start_q=1, test='adf', max_p=5, max_q=5, m=1, d=None, seasonal=False, start_P=0, D=0,\n",
        "                 trace=True, error_action='ignore', suppress_warnings=True, stepwise=True)\n",
        "print(mod.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjf99S3SYD5T",
        "colab_type": "text"
      },
      "source": [
        "#### Diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNTI6duio4Hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auto.plot_diagnostics(figsize=(22,10))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tCCfdFSYJeL",
        "colab_type": "text"
      },
      "source": [
        "### Facebook Prophet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ypytP_OYNne",
        "colab_type": "text"
      },
      "source": [
        "#### Fast Fourier Transformation (FFT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyQKyOrIabdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_fft(data, original_data, smooth_v1=0, smooth_v2=4, opt_fourier=2):\n",
        "  fft = np.fft.fft(data) # fast fourier transform of signal\n",
        "  psd = np.abs(fft)**2 # power spectral density of fourier transform\n",
        "  fftfreq = np.fft.fftfreq(len(psd)) # get frequencies corresponding to the values of the psd\n",
        "  pos = fftfreq > 0 # only interested in positive frequencies -- mirror image if we keep the whole thing\n",
        "  \n",
        "  # find top fundamental frequencies\n",
        "  top_psd = (-psd[pos]).argsort()[:10] # idx=0 is maximal, 1 is 2nd highest, etc.\n",
        "  smooth1 = top_psd[smooth_v1] + 1 # choose the value we want to use as our approximation -- add 1 to get correct index\n",
        "  smooth2 = top_psd[smooth_v2] + 1\n",
        "  \n",
        "  fft1, fft2, fft3 = fft.copy(), fft.copy(), fft.copy()\n",
        "  fft1[np.abs(fftfreq) > fftfreq[smooth1]] = 0 # cut freuencies higher than the fundamental frequency\n",
        "  fft2[np.abs(fftfreq) > fftfreq[smooth2]] = 0\n",
        "  fft3[np.abs(fftfreq) > fftfreq[opt_fourier]] = 0 # hard-code the results from Prophet for comparison\n",
        "  \n",
        "  slow1 = np.real(np.fft.ifft(fft1)) # inverse FFT to convert the modified fourier transform back to temporal domain\n",
        "  slow2 = np.real(np.fft.ifft(fft2))\n",
        "  slow3 = np.real(np.fft.ifft(fft3))\n",
        "  \n",
        "  reformed = reform_data(original_data, data)\n",
        "  slow_ref1, slow_ref2, slow_ref3 = reform_data(original_data, slow1), reform_data(original_data, slow2), reform_data(original_data, slow3)\n",
        "  \n",
        "  # plot results\n",
        "  plt.figure(figsize=(22,12))\n",
        "  plt.subplot(2,1,1)\n",
        "  plt.plot(fftfreq[pos], psd[pos])\n",
        "  plt.axvline(x=fftfreq[smooth1], color='salmon', linestyle=':', label=\"Peak Fundamental Frequency ({})\".format(smooth1))\n",
        "  plt.axvline(x=fftfreq[smooth2], color='green', linestyle=':', label=\"Fundamental Frequency ({})\".format(smooth2))\n",
        "  plt.axvline(x=fftfreq[opt_fourier], color='violet', linestyle=':', label=\"Fundamental Frequency ({})\".format(opt_fourier))\n",
        "  plt.xlabel(\"Frequency [Hz]\")\n",
        "  plt.ylabel(\"Power Spectral Density\")\n",
        "  plt.title(\"Power Spectral Density as a function of frequency\")\n",
        "  plt.legend(fontsize='large')\n",
        "  \n",
        "  plt.subplot(2,1,2)\n",
        "  plt.plot(reformed, label=\"Original Signal\", lw=1.5)\n",
        "  plt.plot(slow_ref1, label=\"FFT ({}) Approx.\".format(smooth1), color='salmon')\n",
        "  plt.plot(slow_ref2, label=\"FFT ({}) Approx.\".format(smooth2), color='green', alpha=0.8)\n",
        "  plt.plot(slow_ref3, label=\"FFT ({}) Approx.\".format(opt_fourier), color='violet', alpha=0.8)\n",
        "  plt.xlabel(\"Trading Day\")\n",
        "  plt.ylabel(\"Closing Price\")\n",
        "  plt.title(\"Original Signal Approximated by Fast Fourier Transforms\")\n",
        "  plt.legend(fontsize='large')\n",
        "  plt.show()\n",
        "  \n",
        "  stats1, stats2, stats3 = performance_stats(reformed, slow_ref1), performance_stats(reformed, slow_ref2), performance_stats(reformed, slow_ref3)\n",
        "  \n",
        "  for idx, stat in zip((smooth1, smooth2, opt_fourier), (stats1, stats2, stats3)):\n",
        "    print(\"\\nFFT ({})\".format(idx))\n",
        "    for key in [\"Projected Sharpe\", \"RMSE\"]:\n",
        "      print(\"{}: {:.4f}\".format(key, stat.get(key)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyFwy3U4acOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_fft(log_diff[:-63], stock_data, smooth_v1=0, smooth_v2=1, opt_fourier=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bHwE293brxN",
        "colab_type": "text"
      },
      "source": [
        "#### Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOthh0n2cUgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pf_analysis(data, period, f_order=8, pred_period=128, mode='additive', tune=True, plot_forecast=False, print_res=False):\n",
        "  # period/fourier analysis\n",
        "  split_date = str( (max(data.index) - timedelta(days=pred_period)).date() )\n",
        "  \n",
        "  # split data into train/test\n",
        "  train, test = data[:split_date].iloc[:-1,], data[split_date:]\n",
        "  \n",
        "  # prediction period is the duration from our split date to the end of our timeseries\n",
        "  pred_per = len(pd.date_range(split_date, max(data.index)))\n",
        "  \n",
        "  # convert training set to a workable format for `Prophet`\n",
        "  df = train.reset_index()\n",
        "  df.columns = ['ds','y']\n",
        "  \n",
        "  # fit Prophet model\n",
        "  if tune:\n",
        "    m = Prophet(interval_width=0.95, daily_seasonality=False, weekly_seasonality=False, yearly_seasonality=False)\n",
        "    m.add_seasonality('self_defined_seasonality', period=period, fourier_order=f_order, mode=mode)\n",
        "    #m.add_seasonality('week', period=5, fourier_order=2, mode=mode)\n",
        "    #m.add_seasonality('month', period=21, fourier_order=7, mode=mode)\n",
        "    #m.add_seasonality('quarter', period=63, fourier_order=31, mode=mode)\n",
        "    #m.add_seasonality('semi_annual', period=126, fourier_order=45, mode=mode)\n",
        "  else: m = Prophet(interval_width=0.95, daily_seasonality=True)\n",
        "  #m2 = m\n",
        "  m.fit(df)\n",
        "  #_rmse = fit_prophet(m2, df)\n",
        "  #print(\"CV Result: {}\".format(_rmse))\n",
        "  \n",
        "  # make predictions with Prohpet model\n",
        "  future = m.make_future_dataframe(periods=pred_per)\n",
        "  forecast = m.predict(future)\n",
        "  \n",
        "  # built-in plotting timeseries and forecast\n",
        "  if plot_forecast:\n",
        "    m.plot(forecast)\n",
        "    plt.plot(test.index, test.values, '.', color='red', alpha=0.6)\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.title(\"Price predicitons with period of {period} days and fourier {fourier}\".format(period=period, fourier=f_order))\n",
        "    plt.show()\n",
        "  \n",
        "  model_tb = forecast['yhat'] # predictions\n",
        "  model_tb.index = forecast['ds'].map(lambda x: x.strftime(\"%Y-%m-%d\")) # corresponding date\n",
        "  out_tb = pd.concat([test, model_tb], axis=1) # join actual values with predictions\n",
        "  out_tb = out_tb[~out_tb.iloc[:,0].isnull()] # drop missing values\n",
        "  out_tb = out_tb[~out_tb.iloc[:,1].isnull()]\n",
        "  out_tb = np.exp(out_tb)\n",
        "  \n",
        "  if plot_forecast:\n",
        "    plt.figure(figsize=(22,8))\n",
        "    out_tb[\"Close\"].plot(label=\"Actual\")\n",
        "    out_tb[\"yhat\"].plot(color=\"red\", label=\"Predicted\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Closing Price\")\n",
        "    plt.title(\"Prophet model with period of {} days and fourier {}\".format(period, f_order))\n",
        "    plt.legend(fontsize='large')\n",
        "    plt.show()\n",
        "  \n",
        "  stats = performance_stats(out_tb.iloc[:,0], out_tb.iloc[:,1])\n",
        "  \n",
        "  if print_res:\n",
        "    for key, value in stats.items():\n",
        "      print(\"{metric}: {stat:.4f}\".format(metric=key, stat=value))\n",
        "  \n",
        "  return stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI-FKh0kcVj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimal_pf(data, pred_per=128, p_low=1, p_high=301, f_low=2, f_high=20):\n",
        "  period_vals, fourier_vals = range(p_low, p_high), range(f_low, f_high)\n",
        "  best_period, best_fourier, min_rmse = p_low, f_low, np.inf\n",
        "  \n",
        "  #print(\"Period\\t|Fourier\\t|RMSE\")\n",
        "  #print(\"===================================\")\n",
        "  for period in period_vals:\n",
        "    for fourier in fourier_vals:\n",
        "      _rmse = pf_analysis(data, period=period, f_order=fourier, pred_period=pred_per).get(\"RMSE\")\n",
        "      #print(\"{}\\t|\\t{}\\t|{:.4f}\".format(period, fourier, _rmse))\n",
        "      if _rmse < min_rmse: best_period, best_fourier, min_rmse = period, fourier, _rmse\n",
        "        \n",
        "  return pf_analysis(data, period=best_period, f_order=best_fourier, pred_period=pred_per, plot_forecast=True, print_res=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzojL6E6cbZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# perform grid search to evaluate optimal hyperparameters by minimizing RMSE\n",
        "p_low, p_high = 225, 230 # 225 to 229\n",
        "f_low, f_high = 1, 5 # 2 to 4 -- values over 5 appear to be overfitting the test set, causing RMSE to be higher\n",
        "pred_per = 128 # 4/26/18 should be start of test period -- 91 trading days equivalent to 128 total days\n",
        "\n",
        "ca_best = optimal_pf(log_data, pred_per, p_low, p_high, f_low, f_high)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HYZrQbUYi5Y",
        "colab_type": "text"
      },
      "source": [
        "### Fama-French Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXk_pfdtZfEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_fama_french(num_factors=5):\n",
        "  # move outside so we only have to load data once\n",
        "  ff_data = pdr.famafrench.get_available_datasets()[num_factors-1] # idx 2 for 3-factor, idx 4 for 5-factor\n",
        "  ff_df = pdr.data.DataReader(ff_data, 'famafrench')[0]\n",
        "  ff_df = ff_df/100\n",
        "  ff_df = ff_df.rename(columns = {'Mkt-RF':'mkt'})\n",
        "  #ff_df = ff_df.drop(['RF'], axis=1) # don't need risk-free rate for algorithm\n",
        "  return ff_df\n",
        "\n",
        "def fama_french_model(ff_data, stock=\"SPY\", start_date='2015-10-31', end_date='2018-9-1', prediction_period=91, bias=1e-4):\n",
        "  #print(\"--------Computing Predictions for {stock}--------\".format(stock=stock))\n",
        "  \n",
        "  # load stock data from specified equity\n",
        "  try: stock_data = load_stock(stock, start_date, end_date)['Close'] # push log difference to later\n",
        "  except KeyError: print(\"* failed loading {} data *\".format(stock))\n",
        "  \n",
        "  # combine data together\n",
        "  df = pd.concat([ff_data, stock_data], axis=1).dropna()\n",
        "  df['Close'] = np.log(df['Close']).diff()\n",
        "  df.dropna(inplace=True)\n",
        "  \n",
        "  # split data into train/test sets\n",
        "  train_set, test_set = df[:-prediction_period], df[-prediction_period:]\n",
        "  # split data into x/y\n",
        "  X_train, y_train = train_set.drop('Close', axis=1), train_set['Close']\n",
        "  X_test, y_test = test_set.drop('Close', axis=1), test_set['Close']\n",
        "  \n",
        "  # fit model on training data\n",
        "  model = lm.LinearRegression().fit(X_train, y_train)\n",
        "  y_preds = model.predict(X_test) + bias # predict on test set, and add a small amount of bias to leverage predictions upward\n",
        "  \n",
        "  Y_true = reform_data(stock_data, df['Close'].values)[-prediction_period:]\n",
        "  Y_pred = reform_data(stock_data, np.concatenate((y_train.values, y_preds)))[-prediction_period:]\n",
        "\n",
        "  # plot our predictions versus actual values\n",
        "  plt.figure(figsize=(22,8))\n",
        "  plt.plot(Y_pred, color='red', label='Predictions')\n",
        "  plt.plot(Y_true, alpha=0.8, label='Actual')\n",
        "  plt.title(\"Returns for \" + stock)\n",
        "  plt.xlabel(\"Days from current date\")\n",
        "  plt.ylabel(\"Return (%)\")\n",
        "  plt.legend(fontsize='large')\n",
        "  plt.show()\n",
        "  \n",
        "  stats = performance_stats(Y_true, Y_pred)\n",
        "  \n",
        "  for key, value in stats.items():\n",
        "    print(\"{}: {:.4f}\".format(key, value))\n",
        "  \n",
        "  return stats # [predicted_ret, actual_ret]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmTtNEj6Zix5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load fama french data\n",
        "ff3_data, ff5_data = load_fama_french(3), load_fama_french(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI4asbuYdP-z",
        "colab_type": "text"
      },
      "source": [
        "#### Fama-French 3-Factor Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWWiz-AXZqK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ff3_model = fama_french_model(ff3_data, bias=1.4e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esIRNgpQdYoL",
        "colab_type": "text"
      },
      "source": [
        "#### Fama-French 5-Factor Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtDLTCLeZqt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ff5_model = fama_french_model(ff5_data, bias=1.3e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXqO0vSNdcmT",
        "colab_type": "text"
      },
      "source": [
        "#### Application to S&P 500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpmcUqIbZwjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"2017-1-1\", \"2019-2-28\"\n",
        "def performance_eval(fama_factors=5, universe_size=50, port_size=10, start=start, end=end, prediction_per=30):\n",
        "  \n",
        "  ff_data = load_fama_french(fama_factors)\n",
        "  \n",
        "  def get_returns(fama_factors, ff_data, universe_size, start, end, prediction_per):\n",
        "    print(\"\\nLoading S&P500 from wikipedia.\")\n",
        "    sp500 = save_sp500_tickers()\n",
        "    random.shuffle(sp500) # shuffle stocks to get a good variety\n",
        "    # GOOG/GOOGL are duplicates -- remove one with no voting rights\n",
        "    sp500 = [symbol for symbol in sp500 if symbol not in {\"GOOG\", \"BRK.B\", \"BF.B\"}]\n",
        "    print(\"Generating predictions with {}-factor model.\".format(fama_factors))\n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "    stock_dict = {}\n",
        "    print(\"Stock\\t|| Predicted\\t|| Actual\\t|| MSE\\t\\t|| R2\")\n",
        "    print(\"==================================================================\")\n",
        "    # just look at the top 50 for now -- too much computation time\n",
        "    for stock in sp500[:universe_size]:\n",
        "      stock_dict[stock] = fama_french(ff_data=ff_data, stock=stock, num_factors=fama_factors,\n",
        "                                      start_date=start, end_date=end, prediction_period=prediction_per)\n",
        "    return stock_dict\n",
        "  \n",
        "  stock_dict = get_returns(fama_factors, ff_data, universe_size, start, end, prediction_per)\n",
        "    \n",
        "  def find_top_returns(stocks, port_size, sort_by_preds=True):\n",
        "    # stocks should be a dictionary, port_size is the number of stocks we want to keep in our portfolio\n",
        "    if sort_by_preds: # sort by predicted value\n",
        "      sorted_stocks = sorted(stocks.items(), key=lambda x: x[1], reverse=True)[:port_size]\n",
        "    else: # sort by actual value\n",
        "      sorted_stocks = sorted(stocks.items(), key=lambda x: x[1][1], reverse=True)[:port_size]\n",
        "    return [x[0] for x in sorted_stocks]\n",
        "  \n",
        "  print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "  predictions = find_top_returns(stock_dict, port_size)\n",
        "  actual = find_top_returns(stock_dict, port_size, False)\n",
        "  print(\"\\nHighest predicted returns:\\n {predictions}\".format(predictions=predictions))\n",
        "  print(\"Highest actual returns:\\n {actual}\".format(actual=actual))\n",
        "  \n",
        "  good_buys = [stock for stock in predictions if stock in actual]\n",
        "  bad_buys = [stock for stock in predictions if stock not in actual]\n",
        "  misses = [stock for stock in actual if stock not in predictions]\n",
        "  print(\"\\nStocks we picked that were actually top performers:\\n {good}\".format(good=good_buys))\n",
        "  print(\"Stocks we picked that were NOT top perfomers:\\n {bad}\".format(bad=bad_buys))\n",
        "  print(\"Stocks we should have picked instead:\\n {miss}\".format(miss=misses))\n",
        "  \n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WkypNnRaCkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creates a 10-stock portfolio from 100 S&P500 symbols using FF5-factor model, based on 1-month predicted returns(%)\n",
        "ff5_preds = performance_eval(universe_size=250)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_WphGKIY23a",
        "colab_type": "text"
      },
      "source": [
        "## Result Summary\n",
        "\n",
        "| Forecasting Model | Est. Returns | Est. Sharpe | RMSE | MAPE | \n",
        "|------------|---------------------|--------------------|-----------|------------| \n",
        "| Persistence | 10.389% | 3.216 | 1.520 | 0.446 |\n",
        "| Random Walk | 10.389% | 3.250 | 1.485 | 0.434 |\n",
        "| Auto-ARIMA | \n",
        "| Facebook Prophet | 9.537% | 23.244 | 2.927 | 0.877 |\n",
        "| Fama-French 3-Factor | 10.209% | 3.087 | 0.596 | 0.168 |\n",
        "| Fama-French 5-Factor | 10.227% | 3.091 | 0.587 | 0.160 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFz2_8tNY7cR",
        "colab_type": "text"
      },
      "source": [
        "## Future Work\n",
        "\n",
        "1. Add economic variables (oil, gold, treasury yields, etc.)\n",
        "2. Add news sentiment\n",
        "3. Neural Network with Fama-French Factors\n",
        "4. Recurrent Neural Network (LSTM)\n",
        "5. Deep Reinforcement Learning\n",
        "6. Ensemble algorithms\n",
        "7. Individualize models by sector\n",
        "8. Extend to entire market\n",
        "9. Automate workflow to purchase stocks at month start"
      ]
    }
  ]
}